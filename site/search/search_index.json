{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto: Pipeline de Dados com Databricks e Delta Lake","text":"<p>Este projeto tem como objetivo demonstrar a implementa\u00e7\u00e3o de um pipeline de dados completo utilizando o Databricks e o Delta Lake, aplicando a Arquitetura Medalh\u00e3o (Medallion Architecture).</p> <p>A arquitetura foi organizada em quatro camadas: Landing, Bronze, Silver e Gold, garantindo uma estrutura modular e escal\u00e1vel para ingest\u00e3o, transforma\u00e7\u00e3o e consumo de dados.</p>"},{"location":"#objetivo","title":"Objetivo","text":"<p>Demonstrar a cria\u00e7\u00e3o e execu\u00e7\u00e3o de um pipeline completo de dados, desde a leitura de arquivos CSV at\u00e9 a gera\u00e7\u00e3o de tabelas anal\u00edticas em formato Delta.</p>"},{"location":"#estrutura-geral-do-projeto","title":"Estrutura Geral do Projeto","text":"<pre><code>trabalho_medalhao/\n\u251c\u2500\u2500 notebooks/\n\u2502 \u251c\u2500\u2500 01_Landing_to_Bronze.ipynb\n\u2502 \u251c\u2500\u2500 02_Bronze_to_Silver.ipynb\n\u2502 \u251c\u2500\u2500 03_Silver_to_Gold.ipynb\n\u2502 \u2514\u2500\u2500 004_Atividade_Pratica_Lakehouse.ipynb\n\u251c\u2500\u2500 docs/\n\u2502 \u251c\u2500\u2500 arquitetura.md\n\u2502 \u251c\u2500\u2500 dataset.md\n\u2502 \u251c\u2500\u2500 notebooks.md\n\u2502 \u2514\u2500\u2500 referencias.md\n\u2514\u2500\u2500 mkdocs.yml\n</code></pre>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Databricks</li> <li>Delta Lake</li> <li>Python (PySpark)</li> <li>MkDocs + Material Theme</li> <li>GitHub</li> <li>VS Code</li> </ul>"},{"location":"#requisitos","title":"Requisitos","text":""},{"location":"#estrutura-do-ambiente-virtual","title":"Estrutura do ambiente virtual:","text":"<p>Crie e ative um ambiente virtual antes de rodar o MkDocs:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate   # no macOS ou Linux\nvenv\\Scripts\\activate      # no Windows\n</code></pre>"},{"location":"#requisitos-de-ambiente","title":"Requisitos de ambiente","text":"<p>Para executar este projeto, \u00e9 necess\u00e1rio possuir:</p> <ul> <li>Conta no Databricks Community Edition ou ambiente empresarial;</li> <li>Python 3.8+ instalado localmente;</li> <li><code>pip</code> e <code>virtualenv</code> configurados;</li> <li>Editor de c\u00f3digo (VS Code recomendado);</li> <li>Acesso a arquivos <code>.csv</code> que comp\u00f5em o dataset.</li> </ul>"},{"location":"#dependencias","title":"Depend\u00eancias","text":"<p>Instale os pacotes necess\u00e1rios:</p> <pre><code>pip install mkdocs mkdocs-material\n</code></pre>"},{"location":"README-original/","title":"README Original","text":""},{"location":"README-original/#projeto-pipeline-de-dados-com-databricks-e-delta-lake-arquitetura-medalhao","title":"Projeto: Pipeline de Dados com Databricks e Delta Lake. Arquitetura Medalh\u00e3o.","text":"<p>Este reposit\u00f3rio apresenta o desenvolvimento de um Pipeline de Dados utilizando o Databricks e o Delta Lake, aplicando a Arquitetura Medalh\u00e3o (Medallion Architecture), com as camadas Landing, Bronze, Silver e Gold.</p> <p>O trabalho foi desenvolvido como parte da disciplina de Engenharia de Dados, com foco em demonstrar o funcionamento completo de um ambiente de processamento de dados moderno e escal\u00e1vel.</p>"},{"location":"README-original/#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>O principal objetivo foi implementar um pipeline orquestrado e funcional, capaz de mover e transformar dados brutos em dados refinados, passando por todas as camadas da Arquitetura Medalh\u00e3o. Al\u00e9m disso, foram avaliados crit\u00e9rios como: - Organiza\u00e7\u00e3o do reposit\u00f3rio no GitHub - Estrutura e clareza dos notebooks Databricks - Funcionamento da Job &amp; Pipeline - Cria\u00e7\u00e3o de documenta\u00e7\u00e3o com MkDocs - Cria\u00e7\u00e3o do README</p>"},{"location":"README-original/#arquitetura-medalhao","title":"Arquitetura Medalh\u00e3o","text":"<p>\u00c9 uma abordagem de camadas para a engenharia de dados, utilizada em ambientes como o Databricks. Cada camada tem uma fun\u00e7\u00e3o espec\u00edfica:</p> Camada Descri\u00e7\u00e3o Tipo de dado Landing \u00c1rea de recep\u00e7\u00e3o dos dados brutos, exatamente como recebidos das fontes. CSV / JSON Bronze Dados brutos convertidos em formato Delta, com tipagem e schema definidos. Raw Delta Silver Dados limpos e tratados, com joins e filtros aplicados. Curated Delta Gold Dados agregados e prontos para consumo anal\u00edtico. Aggregated Delta"},{"location":"README-original/#funcionalidades-e-entregaveis","title":"Funcionalidades e Entreg\u00e1veis","text":"<ul> <li>5 tabelas (m\u00ednimo) em formato CSV ou JSON</li> <li>1 notebook para cada camada (Landing \u2192 Bronze \u2192 Silver \u2192 Gold)</li> <li>Job &amp; Pipeline funcional e orquestrado no Databricks</li> <li>Integra\u00e7\u00e3o com GitHub</li> <li>Documenta\u00e7\u00e3o criada com MkDocs</li> <li>Reposit\u00f3rio organizado e padronizado</li> </ul>"},{"location":"README-original/#execucao-do-pipeline","title":"Execu\u00e7\u00e3o do Pipeline","text":"<ol> <li>Upload dos arquivos brutos para a camada Landing.</li> <li>Execu\u00e7\u00e3o do notebook <code>landing_to_bronze.ipynb</code>, que converte e move os dados para a camada Bronze.</li> <li>Execu\u00e7\u00e3o do notebook <code>bronze_to_silver.ipynb</code>, respons\u00e1vel pelo tratamento e padroniza\u00e7\u00e3o.</li> <li>Execu\u00e7\u00e3o do notebook <code>silver_to_gold.ipynb</code>, que realiza agrega\u00e7\u00f5es e gera dados anal\u00edticos.</li> <li>Job &amp; Pipeline orquestra a execu\u00e7\u00e3o dos tr\u00eas notebooks na sequ\u00eancia.</li> </ol>"},{"location":"README-original/#como-rodar-localmente-documentacao","title":"Como rodar localmente (documenta\u00e7\u00e3o)","text":"<p>Para visualizar a documenta\u00e7\u00e3o com MkDocs:</p> <ol> <li>Crie um ambiente virtual:    ```bash    python3 -m venv venv    source venv/bin/activate  # Mac/Linux    venv\\Scripts\\activate     # Windows</li> </ol> <p>pip install mkdocs mkdocs-material # instala as depend\u00eancias    mkdocs serve # inicia o servidor local    # Acesse o navegador:     http://127.0.0.1:8000</p>"},{"location":"README-original/#alunas-eunice-de-borba-maria-laura-jeronimo-e-vitoria-viana","title":"Alunas: Eunice de Borba, Maria Laura Jeronimo e Vit\u00f3ria Viana","text":""},{"location":"arquitetura/","title":"Arquitetura Medalh\u00e3o","text":"<p>A Arquitetura Medalh\u00e3o \u00e9 uma abordagem amplamente usada em engenharia de dados moderna, com o objetivo de organizar e refinar os dados progressivamente em camadas.</p>"},{"location":"arquitetura/#camadas-do-pipeline","title":"Camadas do Pipeline","text":"Camada Descri\u00e7\u00e3o Tipo de Dados Landing \u00c1rea de recep\u00e7\u00e3o dos dados brutos, como foram recebidos da fonte. CSV / JSON Bronze Convers\u00e3o dos dados para formato Delta, com estrutura definida. Raw Delta Silver Dados tratados, limpos e enriquecidos. Curated Delta Gold Dados anal\u00edticos, prontos para dashboards e relat\u00f3rios. Aggregated Delta"},{"location":"arquitetura/#fluxo-do-pipeline","title":"Fluxo do Pipeline","text":"<ol> <li>Landing \u2192 Bronze: ingest\u00e3o e convers\u00e3o para formato Delta.  </li> <li>Bronze \u2192 Silver: tratamento, limpeza e padroniza\u00e7\u00e3o.  </li> <li>Silver \u2192 Gold: agrega\u00e7\u00f5es e estrutura\u00e7\u00e3o final para an\u00e1lise.</li> </ol>"},{"location":"arquitetura/#beneficios-da-arquitetura-medalhao","title":"Benef\u00edcios da Arquitetura Medalh\u00e3o","text":"<ul> <li>Padroniza\u00e7\u00e3o e controle das etapas de transforma\u00e7\u00e3o.</li> <li>Reprocessamento facilitado.</li> <li>Escalabilidade e reusabilidade de dados.</li> <li>Integra\u00e7\u00e3o direta com ferramentas de BI e Analytics.</li> </ul>"},{"location":"dataset/","title":"Dataset","text":"<p>O projeto utiliza um conjunto de dados simulado que representa informa\u00e7\u00f5es de e-commerce \u2014 clientes, pedidos, produtos e avalia\u00e7\u00f5es.</p>"},{"location":"dataset/#estrutura-dos-arquivos","title":"Estrutura dos Arquivos","text":"Arquivo Descri\u00e7\u00e3o Volume aproximado customers.csv Dados de clientes 10 MB orders.csv Pedidos realizados 25 MB order_items.csv Itens de cada pedido 15 MB products.csv Cadastro de produtos 5 MB reviews.csv Avalia\u00e7\u00f5es de produtos 3 MB"},{"location":"dataset/#localizacao","title":"Localiza\u00e7\u00e3o","text":"<p>Os arquivos s\u00e3o armazenados na Landing Zone, e o caminho base \u00e9 definido em c\u00f3digo como:</p> <pre><code>CATALOG_NAME = \"workspace\"\nSCHEMA_NAME = \"projeto_medalhao\"\nLANDING_PATH = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/landing_zone/\"\n</code></pre>"},{"location":"execucao/","title":"Execu\u00e7\u00e3o do Pipeline","text":"<p>Nesta se\u00e7\u00e3o \u00e9 descrito o processo de execu\u00e7\u00e3o do pipeline do projeto Medalh\u00e3o, composto pelas camadas Landing, Bronze, Silver e Gold.</p>"},{"location":"execucao/#passo-a-passo-de-execucao","title":"Passo a passo de execu\u00e7\u00e3o","text":""},{"location":"execucao/#1-preparacao-do-ambiente","title":"1. Prepara\u00e7\u00e3o do ambiente","text":"<p>Antes de rodar os notebooks, certifique-se de que o ambiente Databricks est\u00e1 configurado corretamente com: - Cluster ativo e compat\u00edvel com PySpark; - Acesso ao Unity Catalog (caso esteja habilitado); - Arquivos CSV dispon\u00edveis na Landing Zone.</p>"},{"location":"execucao/#2-execucao-dos-notebooks","title":"2. Execu\u00e7\u00e3o dos notebooks","text":"<p>O pipeline segue a sequ\u00eancia abaixo:</p> Etapa Notebook Descri\u00e7\u00e3o 1\ufe0f\u20e3 <code>01_Landing_to_Bronze.ipynb</code> Leitura dos arquivos CSV brutos da Landing Zone, padroniza\u00e7\u00e3o e escrita no formato Delta na camada Bronze. 2\ufe0f\u20e3 <code>02_Bronze_to_Silver.ipynb</code> Limpeza, enriquecimento e normaliza\u00e7\u00e3o dos dados. Gera\u00e7\u00e3o das tabelas anal\u00edticas intermedi\u00e1rias na camada Silver. 3\ufe0f\u20e3 <code>03_Silver_to_Gold.ipynb</code> Agrega\u00e7\u00e3o e transforma\u00e7\u00e3o final dos dados para uso em relat\u00f3rios, dashboards e insights de neg\u00f3cio."},{"location":"execucao/#parametros-principais","title":"Par\u00e2metros principais","text":"<ul> <li>CATALOG_NAME: <code>workspace</code></li> <li>SCHEMA_NAME: <code>projeto_medalhao</code></li> <li>LANDING_PATH: Caminho de origem dos arquivos CSV.</li> <li>BRONZE_PATH / SILVER_PATH / GOLD_PATH: Caminhos das tabelas Delta geradas em cada camada.</li> </ul>"},{"location":"execucao/#estrutura-das-camadas","title":"Estrutura das camadas","text":"<p>```text Landing Zone/  \u251c\u2500\u2500 customers.csv  \u251c\u2500\u2500 orders.csv  \u251c\u2500\u2500 order_items.csv  \u251c\u2500\u2500 products.csv  \u2514\u2500\u2500 reviews.csv</p> <p>Bronze/  \u251c\u2500\u2500 bronze_customers.delta  \u251c\u2500\u2500 bronze_orders.delta  \u251c\u2500\u2500 bronze_products.delta  \u2514\u2500\u2500 bronze_reviews.delta</p> <p>Silver/  \u251c\u2500\u2500 silver_orders_enriched.delta  \u251c\u2500\u2500 silver_customers_enriched.delta  \u2514\u2500\u2500 silver_products_enriched.delta</p> <p>Gold/  \u251c\u2500\u2500 gold_sales_summary.delta  \u2514\u2500\u2500 gold_customer_rfm.delta</p>"},{"location":"referencias/","title":"Refer\u00eancias","text":"<p>Abaixo est\u00e3o as principais refer\u00eancias te\u00f3ricas e t\u00e9cnicas utilizadas no desenvolvimento do projeto, que abordam conceitos de Engenharia de Dados, Delta Lake, Databricks e a Arquitetura Medalh\u00e3o.</p>"},{"location":"referencias/#documentacao-oficial","title":"Documenta\u00e7\u00e3o Oficial","text":"<ul> <li> <p>Databricks Documentation \u2014 Workflows e Data Engineering   Guia oficial com detalhes sobre notebooks, jobs e pipelines no Databricks.   \ud83d\udd17 https://docs.databricks.com/en/workflows/index.html</p> </li> <li> <p>Delta Lake Documentation \u2014 Lakehouse Architecture   Descri\u00e7\u00e3o t\u00e9cnica do Delta Lake e sua integra\u00e7\u00e3o com a arquitetura em camadas.   \ud83d\udd17 https://docs.delta.io/latest/delta-intro.html</p> </li> <li> <p>Apache Spark \u2014 Structured Data Processing   Base para o processamento distribu\u00eddo de dados usado no Databricks.   \ud83d\udd17 https://spark.apache.org/docs/latest/sql-programming-guide.html</p> </li> </ul>"},{"location":"referencias/#artigos-e-guias-tecnicos","title":"Artigos e Guias T\u00e9cnicos","text":"<ul> <li> <p>Medallion Architecture in Databricks (Official Blog)   Artigo que explica o conceito e aplica\u00e7\u00e3o pr\u00e1tica da Arquitetura Medalh\u00e3o.   \ud83d\udd17 https://www.databricks.com/blog/2022/05/12/what-is-a-medallion-architecture.html</p> </li> <li> <p>Delta Lake: The Definitive Guide (Databricks Blog)   Explica como o Delta Lake otimiza confiabilidade e desempenho em pipelines de dados.   \ud83d\udd17 https://www.databricks.com/blog/2020/06/17/the-delta-lake-definitive-guide.html</p> </li> </ul>"},{"location":"resultados/","title":"Resultados","text":""},{"location":"resultados/#docsresultadosmd","title":"docs/resultados.md","text":"<p>```markdown</p>"},{"location":"resultados/#resultados","title":"Resultados","text":"<p>Ap\u00f3s a execu\u00e7\u00e3o completa do pipeline, o projeto gera dados organizados e prontos para an\u00e1lise nas camadas Bronze, Silver e Gold.</p>"},{"location":"resultados/#camada-bronze","title":"\ud83d\udd0d Camada Bronze","text":"<p>Cont\u00e9m os dados brutos padronizados, convertidos para formato Delta e com metadados de origem e timestamp de ingest\u00e3o.</p> <p>Exemplo de schema: ```text bronze_customers \u251c\u2500\u2500 customer_id \u251c\u2500\u2500 name \u251c\u2500\u2500 email \u251c\u2500\u2500 source_file \u2514\u2500\u2500 ingestion_timestamp</p>"},{"location":"usage/","title":"Como usar","text":""},{"location":"usage/#passos-para-visualizar-a-documentacao-localmente","title":"Passos para visualizar a documenta\u00e7\u00e3o localmente","text":"<ol> <li>Abra o terminal na pasta do projeto:</li> </ol> <p><code>bash    cd trabalho_medalhao</code></p> <ol> <li>Crie e ative um ambiente virtual:</li> </ol> <p><code>bash    python3 -m venv venv    source venv/bin/activate  # no macOS/Linux</code></p> <ol> <li>Instale o MkDocs e o tema Material:</li> </ol> <p><code>bash    pip install mkdocs mkdocs-material</code></p> <ol> <li>Rode o site localmente:</li> </ol> <p><code>bash    mkdocs serve</code></p> <ol> <li>Abra no navegador:</li> </ol> <p>http://127.0.0.1:8000</p> <ol> <li>Para publicar no GitHub Pages:</li> </ol> <p><code>bash    mkdocs gh-deploy</code></p>"}]}